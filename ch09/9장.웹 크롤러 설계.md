# 9장 웹 크롤러 설계

## 🏁 intro

### ✅ 웹 크롤러 ?

- 로봇, 스파이더
- 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적
- 콘텐츠 : 웹 페이지, 이미지, 비디오, PDF 파일 .. 등
- 웹 크롤러의 복잡도는 데이터 규모에 따라 천차만별
    ![image1](https://github.com/user-attachments/assets/9d3baafa-1265-4a19-a5ea-502aa2d0dce4)


### ✅ 크롤러의 이용 및 용도

- 검색 엔진 인덱싱 search engine indexing
- 웹 아카이빙 web archiving
- 웹 마이닝 web mining
- 웹 모니터링 web monitoring

## 🏁 1단계 | 문제 이해 및 설계 범위 확정

### ✅ 웹 크롤러의 기본 알고리즘

1. URL 집합이 입력 > 해당 URL 가리키는 모든 웹페이지 다운로드
2. 다운로드된 웹페이지에서 URL들을 추출
3. URL 목록에 추출된 URL 추가
4. 1번 반복

⇒ 실은 이렇게 단순하지 않다

### ✅ 질문으로 요구사항 파악 및 설계 범위 좁히기

```
🙋 : 크롤러의 주된용도 ? 
👨‍💻 : 검색 엔진 인덱싱

🙋 : 매달 얼마나 많은 웹페이지 수집? 
👨‍💻 : 10억 개

🙋 : 수정된 웹 페이지도 고려 ?
👨‍💻 : yes

🙋 : 수집한 웹 페이지 저장 필요 ? 
👨‍💻 : yes 5년간 저장

🙋 : 중복 콘텐츠는 ? 
👨‍💻 : 무시

=> 면접관의 마음과 나의 마음을 일치 시려라!
```

### ✅ 웹 크롤러의 속성

- 규모 확장성
- 안정성
- 예절 - 싸가지 ?
- 확장성
    
    ⇒ 웹 크롤러의 속성을 고려하여 설계 하자!
    

### ✅ 개략적 규모 추정

🗣️ 5년 동안 얼마 만큼의 저장용량이 필요한지 구해보세요

- 정답 보기
    - 매달 10억 개의 웹 페이지를 다운로드한다.
    - QPS = 10억/30일/24시간/3600초=대략 400페이지/초
    - 최대 QPS = 2 * QPS = 800
    - 웹 페이지의 크기 평균은 500k라고 가정
    - 10억 페이지*500k=500TB/월.
    - 1개월치가 500TB이므로, 5년치를 보관한다면 결국 500TB*12개월*5년=30PB의 저장용량이 필요하다.

## 🏁 2단계 | 개략적인 설계안 제시 및 동의 구하기

## 📍 컴포넌트 설명 📍
![image2](https://github.com/user-attachments/assets/c09f1200-256d-46f2-8435-b1a6d91db1ea)


### ✅ 시작 URL 집합

- 크롤링을 시작하는 출발점
- 크롤러가 가능한 많은 링크를 탐색 할 수 있도록 하는 URL 전략
- 전체 URL 공간을 작은 부분집합으로 나누는 전략
- 주제별로 다른 시작 URL 사용하는 전략

### ✅ 미수집 URL 저장소(URL frontier)

- 다운로드할 URL 을 관리하는 컴포넌트

<aside>
💡

크롤링 상태는 오직 두개 뿐

 1) 다운로드할 URL, 2) 다운로드된 URL

</aside>

### ✅ HTML 다운로더

- 인터넷에서 웹페이지를 다운로드

### ✅ 도메인 이름 변환기

- URL → IP

### ✅ 콘텐츠 파서

- 파싱과 검증
- 독립적인 컴포넌트 - 크롤링 과정이 느려짐을 방지

### ✅ 중복 콘텐츠 인가 ?

- 데이터 중복을 줄임
- 데이터 처리 시간 줄임
- 웹 페이지의 해시 값을 비교 - 가장 효과적

### ✅ 콘텐츠 저장소

- HTML 보관하는 시스템
- 데이터의 유형, 크기, 저장소 접근 빈도, 데이터 유효기간을 종합적으로 고려
- 본 설계안의 경우는 어떤 저장소를 택해야 할까?
    - 정답 ?
        
        디스크와 메모리를 모두 사용하는 저장소
        
        - 데이터 양이 많기에 대부분 콘텐츠 디스크에 저장
        - 인기 콘텐츠는 메모리에 두어 접근 지연시간을 줄임

### ✅ URL 추출기

- HTML 페이지 파싱하여  링크 추출
- 상대 경로 → 절대 경로
    ![image3](https://github.com/user-attachments/assets/ffc31289-9deb-4fd2-8be6-99ffe5f96893)

    

### ✅ URL 필터

- 크롤링 대상에서 배제
    - 특정 콘텐츠 타입 혹은 확장자를 갖는 URL
    - 접속 시 오류 발생 URL
    - 접근 제외 목록 URL

### ✅ 이미 방문한 URL ?

- 미수집 URL 저장소에 보관된 URL 추적
- 서버 부하 줄이기
- 무한 루프 방지
- 자료 구조 : 블룸 필터, 해시 테이블

### ✅ URL 저장소

- 이미 방문한 URL 보관

## 📍 웹 크롤러 작업 흐름 📍
![image4](https://github.com/user-attachments/assets/3cd802df-c6fa-40d1-8e57-8eb146bdb706)


1. 시작 URL들을 미수집 URL 저장소에 저장한다.
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운받는다.
⇒ 도메인 이름 변환기는 동기적 특성으로 성능 최적화 필요하다 → 캐시에 저장 (뒤에 나옴) 
4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시한다.
6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.
    - 이미 저장소에 있는 콘텐츠인 경우에는 처리하지 않고 버린다.
    - 저장소에 없는 콘텐츠인 경우 저장소에 저장한 뒤 URL 추출기로 전달
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라냄
8. 골라낸 링크를 URL 필터로 전달
9. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달
10. 이미 처리한 URL인지 확인하기 위하여 URL 저장소에 보관된 URL인지 판단. 이미 저장소에 있는 URL은 버린다
11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달한다.

## 🏁 3단계 | 상세 설계 🌟 - 컴포넌트와 구현기술

### ✅ DFS / BFS

- 웹은 유향 그래프
    - 페이지 - 노드
    - 하이터링크URL - 엣지
- DFS 비추 - 그래프가 클 경우 깊이 가늠하기 어려움
- BFS - FIFO를 사용하는 알고리즘
    - 문제점 두 가지 ?
        1. 링크 대부분 같은 서버로 되돌아간다 
            - 같은 호스트에 속한 링크를 다운로드 → 서버는 수많은 요청으로 과부하
            - 예의 없는 크롤러가 된다
              ![image11](https://github.com/user-attachments/assets/847087f2-5bd1-497b-b35f-e13ea4973b78)


            
        2. 우선순위가 없음
            - 페이지 순위, 사용자 트래픽의 양, 업데이트 빈도 등 우선순위를 구별하는 것이 좋다

### ✅ 미수집 URL 저장소

- 다운로드할 URL을 보관
    - 예의
    - 우선순위
    - 신선도
    - 미수집 저장소의 구현

📌 **예의**

- 예의 짤  
  ![image6](https://github.com/user-attachments/assets/101bd85c-236d-434f-863c-002814382d04)
    
- 무례한 : 시간대비 너무 많은 요청. DoS 공격
- 동일한 웹사이트에 대해서는 한 번에 한페이지만 요청한다(원칙)
같은 웹 사이트 페이지를 다운받는 테스크는 시간차를 두고 실행
    
    ⇒ 호스트명, 작업스레드 사이의 관계 유지
    ![image5](https://github.com/user-attachments/assets/08cc4297-63a1-469d-aae4-8405f01941d5)

    
    `큐 라우터` : 같은 호스트에 속한 URL 은 언제나 같은 큐에 보장
    
    `매핑 테이블` : 호스트 이름과 큐의 관계 보관 테이블
    
    | 호스트 | 큐 |
    | --- | --- |
    | wikipedia.com | b1 |
    | apple.com | b2 |
    
    `FIFO 큐` : 같은 호스트에 속한 URL 은 같은 큐에 보관
    
    `큐 선택기` : 큐 순회하면서 큐에서 URL을 꺼내서 지정된 작업스레드에 전달
    
    `작업 스레드` : 전달된 URL 다운로드 하는 작업 
    

📌 **우선순위**
![image7](https://github.com/user-attachments/assets/355713e1-eed4-419d-8be3-d34d282b74f9)


`순위결정장치` : URL 입력받아 우선순위 계산

`큐(f1 .. fn)` : 우선순위별로 큐가 하나씩 할당

`큐 선택기` : 큐에서 URL 꺼내기. 순위가 높은 큐에서 더 자주 꺼냄
![image8](https://github.com/user-attachments/assets/c010e70a-3f95-451b-8e79-b2683db4b65a)


- 전면 큐 : 우선순위 결정과정을 처리
- 후면 큐 : 크롤러가 예의 바르게 동작하도록 보장

📌 **신선도**

- 재수집 필요가 있다
1. 변경이력을 활용
2. 우선순위에 따라 중요 페이지 자주 수집

📌 **미수집 URL 저장소를 위한 지속성 저장장치**

- URL은 디스크에, 큐는 메모리 버퍼에 ..

### ✅ HTML 다운로더

📌 **Robots.txt**

- 크롤러가 수집해도 되는 페이지 목록
- 캐시에 보관
- https://www.amazon.com/robots.txt → 다운받지 못하는 것을 찾아보세요

📌 **HTML 다운로더에서 쓸 수 있는 성능최적화 방안**

1. 분산 크롤링 - 크롤링 작업을 여러 서버에 분산하기
    ![image9](https://github.com/user-attachments/assets/e1c48b3e-fdb2-456c-a582-02540e244159)

    
2. 도메인 이름 변화 결과 캐시
    - 크롤러 성능의 병목 중 하나
    - DNS 요청 보내고 결과받는 작업의 동기적 특성 10ms ~ 200ms
    - 도메인 이름과 IP 주소 사이의 관계를 캐시에 저장 → 크론잡 주기적으로 돌리기
        
        <aside>
        💡
        
        ***CronJob***
        
        주기적으로 특정 동작을 수행하고 종료하는 작업(배치 작업)을 정의하기 위한 리소스이다
        
        </aside>
        
3. 지역성 - 크롤링 작업을 수행하는 서버를 지역별로 분산하기
4. 짧은 타임아웃 - 응답 느리거나 응답없는 웹페이지 out

📌 **HTML 다운로더에서 쓸 수 있는 안정성 방안**

- 안정해시 - 서버 부하 분산시 적용가능
- 크롤링 상태 및 수집 데이터 저장 - 장애 발생시 쉽게 복구 가능
- 예외 처리
- 데이터 검증 - validation

📌 **확장성**

- 새로운 콘텐츠 쉽게 지원
  ![image10](https://github.com/user-attachments/assets/081f4c30-2f85-40af-ba84-f6a6d283bf07)


- 위 그림은 확장 모듈 끼우기
    - PNG 다운로더 - PNG 파일 다운로드하는 플러그인 모듈
    - 웹 모니터 - 웹 모니터링 하여 저작권이나 상표 침해 막기

📌 **문제 있는 콘텐츠 감지 및 회피 방안**

1. 중복 콘텐츠 회피 - 해시나 체크섬으로 대응
    
    <aside>
    💡
    
    - 웹 콘텐츠는 몇% 중복일까 ?
        
        30%
        
    </aside>
    
2. 거미 덫 - 크롤러를 무한 루프에 빠뜨리도록
- URL 길이 제한두기
- 수작업으로 목록을 만들어서 URL 필터 목록에 걸어두기
1. 데이터 노이즈 - 가치없는 데이터 ex. 광고, 스팸

## 🏁 4단계 | 마무리

### ✅ 좋은 크롤러가 가져야할 특성

- 규모 확장성
- 예의
- 확장성
- 안정성

### ✅ 설계안, 핵심 컴포넌트의 기술들

### ✅ 추가 논의

- SSR - 페이지를 파싱전에 SSR 를 적용
- 원치 않는 페이지 필터링 - 스팸 방지 컴포넌트 추가
- 데이터베이스 다중화 및 샤딩 - 데이터 계층의 가용성, 규모 확장성, 안정성 이 향상
- 수평적 규모 확장성 - 무상태. 서버가 상태 정보를 유지하지 않도록 하는 것
